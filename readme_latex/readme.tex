\documentclass{article}
\usepackage[margin=2cm]{geometry}
\usepackage{amsmath} % for math notation
\usepackage{enumerate} % for customizing lists
\usepackage{mathtools}
\usepackage{listings}
\usepackage[T1]{fontenc}
\usepackage{tikz}
\usepackage{graphicx}
\usepackage{float}
\usetikzlibrary{shapes,arrows}

\usepackage [english]{babel}
\usepackage [autostyle, english = american]{csquotes}

\title{COMP 4321 - Project Report and Documentation}
\author{Leung Ka Wa, 20770807, kwleungau@connect.ust.hk}
\date{}

\begin{document}
    \maketitle
    \section*{Program code Structure}
        \subsection*{java source code (project\_package)}
            \begin{itemize}
                \item \textbf{Crawler.java} - Crawler class
                \item \textbf{StopStem.java} - StopStem class
                \item \textbf{URLIndex.java} - URLIndex class, use to manipulate the URL.db
                \item \textbf{WordIndex.java} - WordIndex class, use to manipulate the WordDB.db
                \item \textbf{Tester.java} - Tester class, use to test and run the crawler.
                \item \textbf{SearchEngine.java} - SearchEngine class, use to perform IR.
            \end{itemize}
        \subsection*{Library}
            No extra library from lab is used in this project.

    \section*{Design of the jdbm database scheme}

    \subsection*{URL.db}
    It contain of 6 objects. Each of them is a HTree.
    \begin{itemize}
        \item \textbf{PageID} - Store the URL and its pageID. \\[0.4em]
        (\textbf{String})URL = (\textbf{UUID})pageID. Example:\\[0.4em]
        http://library.hkust.edu.hk/events/staff-workshops/ = b9275b04-58a1-3f4f-ab38-606e30a198a8 \\[0.4em]
        Design decision: A ID mapping. 

        \item \textbf{ParentToChilden} - Store the parent to children relationship. \\[0.4em]
        (\textbf{UUID})parentID = Vector$\langle$\textbf{UUID}$\rangle$(childID). Example:\\[0.4em]
        5ed456f8-36f3-3ca2-8451-62696e13f7fc = [b9275b04-58a1-3f4f-ab38-606e30a198a8, 9e4a5a31-dbb7-39d0-82ab-8d8b37595564, bd93a542-88ec-3b29-b739-9faf1ffc3bdc, \dots] \\[0.4em]
        Design decision: Can easily get the out link of a page for later use, e.g. PageRank, hub weight, authority weight and HITS.
        
        \item \textbf{ChildToParents} - Store the child to parents relationship. \\[0.4em]
        (\textbf{UUID})childID = Vector$\langle$\textbf{UUID}$\rangle$(parentID). Example:\\[0.4em]
        5ed456f8-36f3-3ca2-8451-62696e13f7fc = [b9275b04-58a1-3f4f-ab38-606e30a198a8, 9e4a5a31-dbb7-39d0-82ab-8d8b37595564, bd93a542-88ec-3b29-b739-9faf1ffc3bdc, \dots] \\[0.4em]
        Design decision: Can easily get the in link of a page for later use, e.g. PageRank, hub weight, authority weight and HITS.

        \item \textbf{PageToTitle} - Store the pageID and its originial title. \\[0.4em]
        (\textbf{UUID})pageID = (\textbf{String})title. Example:\\[0.4em]
        bd93a542-88ec-3b29-b739-9faf1ffc3bdc = "This is the Title"\\[0.4em]
        Design decision: Store the whole title for display use only.

        \item \textbf{LastModified} - Store the pageID and its last modified date. \\[0.4em]
        (\textbf{UUID})pageID = (\textbf{Date})lastModifiedDate. Example:\\[0.4em]
        bd93a542-88ec-3b29-b739-9faf1ffc3bdc = (\textbf{Date})Thu Jun 16 16:47:33 HKT 2022\\[0.4em]
        Design decision: Store the last modified date of a page to determine whether the page is updated or not.
        
        \item \textbf{PageSize} - Store the pageID and its size. \\[0.4em]
        (\textbf{UUID})pageID = (\textbf{Integer})size. Example:\\[0.4em]
        bd93a542-88ec-3b29-b739-9faf1ffc3bdc = 1024\\[0.4em]
        Design decision: Store the size of a page to determine how much infomation are in this page.
    \end{itemize}


    \subsection*{WordDB.db}
    It contain of 4 objects. Each of them is a HTree.
    \begin{itemize}
        \item \textbf{WordID} - Store the word and its ID. \\[0.4em]
        (\textbf{String})word = (\textbf{UUID})wordID. Example:\\[0.4em]
        intellig = b9275b04-58a1-3f4f-ab38-606e30a198a8 \\[0.4em]
        Design decision: A ID mapping.
        \item \textbf{Inverted} - Store the wordID and posting list. \\[0.4em]
        (\textbf{UUID})wordID = Map$\langle$(\textbf{UUID})pageID, Vector$\langle$\textbf{Integer}$\rangle$(position)$\rangle$.
        Example:\\[0.4em]
        b9275b04-58a1-3f4f-ab38-606e30a198a8 = \{9bfc960c-53e4-3faf-8623-b44c251584c1=[1, 5, 10], 114471e0-e3dd-39d8-aa8a-11f77c85a7fa=[50, 60], 8019de9c-bcf5-3600-814b-53ed90ab33bb=[10], \dots \}\\[0.4em]
        Design decision: Store the posting list of the word for tfxidf and phase search. Also, finding the document with highest word frequency is easy.
        \item \textbf{Forward} - Store the pageID and its forward word list. \\[0.4em]
        (\textbf{UUID})pageID = Map$\langle$(\textbf{UUID})wordID, Vector$\langle$\textbf{Integer}$\rangle$(position)$\rangle$.
        Example:\\[0.4em]
        b9275b04-58a1-3f4f-ab38-606e30a198a8 = \{9bfc960c-53e4-3faf-8623-b44c251584c1=[1, 5, 10], 114471e0-e3dd-39d8-aa8a-11f77c85a7fa=[50, 60], 8019de9c-bcf5-3600-814b-53ed90ab33bb=[10], \dots \}\\[0.4em]
        Design decision: Store the forward word list of the page for later algorithm, e.g. tfxidf. Finding the words and their position to get the phase and frequency in a document is easy.
        \item \textbf{TitleInverted} - Store the wordID and posting list of title. \\[0.4em]
        (\textbf{UUID})wordID = Map$\langle$(\textbf{UUID})pageID, Vector$\langle$\textbf{Integer}$\rangle$(position)$\rangle$.
        Example:\\[0.4em]
        b9275b04-58a1-3f4f-ab38-606e30a198a8 = \{9bfc960c-53e4-3faf-8623-b44c251584c1=[1, 5, 10], 114471e0-e3dd-39d8-aa8a-11f77c85a7fa=[50, 60], 8019de9c-bcf5-3600-814b-53ed90ab33bb=[10], \dots \}\\[0.4em]
        Design decision: Store the posting list of the word in title to favor matches in title.
    \end{itemize}

    \newpage
    \section*{Running the program (crawler part)}
    \subsection*{How to run the program}
    
        The Tester class is the main calling class of this program. Pass command line argument to it to run the program. \\[0.4em]
        As I am using VS code to develop this project, I was just simply using the java extension and run the program without mannually compile the project. 
        For me the command line is:
        \begin{lstlisting}[language=bash,breaklines=true]
            /usr/bin/env /Users/boscoleung/opt/anaconda3/bin/java @/var/folders/f1/6mvnwxt109n9rswystbch0t40000gn/T/cp_dh97avm16bvprxybpew6los8v.argfile project_package.Tester <argument>
        \end{lstlisting}

        If you want to compile the project mannually, you can run the following command (work on mac): \\[0.8em]
        % \centerline{\textbf{javac -cp \texttt{"}:lib/*\texttt{"} -d bin \$(find . -name \texttt{"}*.java\texttt{"})}} 
        \centerline{\textbf{javac -cp \texttt{"}:lib/*\texttt{"} -d bin \$(find . -path ./apache-tomcat-10.1.6 -prune -o -name \texttt{"}*.java\texttt{"} -print)}} \\[0.1em]

        A bin folder containing all the classes will be created.
        \begin{figure}[!htbp]
            \centering
            \label{}
            \includegraphics[width=5cm]{compliedBin.png}
            \caption{The complied bin folder}
        \end{figure}

        Then run the program with the following command: \\[0.8em]
        \centerline{\textbf{java -cp \texttt{"}.:bin:lib/*\texttt{"} project\texttt{\_}package.Tester $<$argument$>$}}

        \begin{itemize}
            \item \textbf{-runCrawler} - Run the crawler, progress will be printed to the console. The starting URL and the number of pages to crawl can be set by add the url and number. For example, \textbf{\texttt{-runCrawler https://www.google.com 1000}}, if no url and number is provided, the default url and number will be used, that is \textbf{\texttt{-runCrawler} $=$ \texttt{-runCrawler https://www.cse.ust.hk/$\sim$kwtleung/COMP4321/testpage.htm 300}};
            \item \textbf{-printSpiderResult} - Output the result of the crawler to spider\_result.txt. This may take a moment to produce the complete result.
            \item \textbf{-printAllURLdb} - Output all the data in the URL.db to AllURLdb.txt.
            \item \textbf{-printPageTitle} - Output all the data in the URL.db PageToTitle to PageTitle.txt.
            \item \textbf{-printURLPageID} - Output all the data in the URL.db PageID to URLPageID.txt.
            \item \textbf{-printPageMeta} - Output all the data in the URL.db LastModified and PageSize to PageMeta.txt.
            \item \textbf{-printParentToChilden} - Output all the data in the URL.db ParentToChildren to ParentToChildren.txt.
            \item \textbf{-printChildToParents} - Output all the data in the URL.db ChildToParents to ChildToParents.txt.
            \item \textbf{-printAllWordDB} - Output all the data in the WordDB.db to AllWordDB.txt.
            \item \textbf{-printWordID} - Output all the data in the WordDB.db WordID to WordID.txt.
            \item \textbf{-printInverted} - Output all the data in the WordDB.db Inverted to Inverted.txt.
            \item \textbf{-printTitleInverted} - Output all the data in the WordDB.db TitleInverted to TitleInverted.txt.
            \item \textbf{-printForward} - Output all the data in the WordDB.db Forward to Forward.txt.
        \end{itemize}

    % Define block styles
    \tikzstyle{decision} = [diamond, draw, fill=blue!20, 
    text width=5em, text badly centered, node distance=3cm, inner sep=0pt]
    \tikzstyle{block} = [rectangle, draw, fill=blue!20, 
    text width=8em, text centered, rounded corners, minimum height=2.5em]
    \tikzstyle{line} = [draw, -latex']
    \tikzstyle{cloud} = [draw, ellipse,fill=red!20, node distance=3cm,
    minimum height=2em]
    
    \newpage
    \section*{Search Engine Pipeline}
    \begin{center}
        \begin{tikzpicture}[node distance = 2cm, auto]
            % Place nodes
            \node [cloud] (input) {input query};
            \node [block, below of=input, node distance=1.5cm, text width=10em] (stomstem) {stop stem every word};

            \node [block, below of=stomstem, text width=35em, node distance=2cm, left] (queryTerm) {calculate query term weight (phase consider as a term) \\
            $w_{term,query}=(0.5+0.5\frac{tf_{term,query}}{max(tf_{query})})\log_2(1+\frac{N_{num\_indexed\_page}}{df_{num\_page\_contain\_term}})$ \\
            noraml query word that havent appear in the database will be ignored.};
            
            \node [block, below of=stomstem, right=0.5cm, node distance=2.5cm, text width=12em] (queryTermException) {if \\ 1. $df_{phase\_word}=0$ \\ 2. phase are all stopped \\ 3. all query not indexed\\4. query are all stopped};

            \node [cloud, below of=queryTermException, node distance=3cm] (noResultEndding) {No Result};

            \node [decision, below of=queryTerm, , node distance=2.75cm] (phaseDecide) {contain phase search?};


            \node [block, left of=phaseDecide, node distance=4cm] (phaseSearch) {phase search};
            \node [block, below of=phaseSearch, text width=20em] (processPhaseSearch1) {obtain the page that contain the all phases with count in body and title respectively};
            \node [block, below of=processPhaseSearch1, text width=20em, node distance = 1.8cm] (processPhaseSearch2) {calculate the term weight of the page\\ $w_{term,page}=(0.5+0.5\frac{tf_{term,page}}{max(tf_{page})})\times$ \\ $\log_2(1+\frac{N_{num\_indexed\_page}}{df_{num\_page\_contain\_term}})$\\ Zero if the term havent appear in the page};
            \node [block, below of=processPhaseSearch2, text width=20em, node distance=2.2cm] (processPhaseSearch3) {favor query word in title by frequency:\\ $w_{term}=w_{term,page}\times(1.25\times tf_{term\_in\_title,page})$};

            \node [block, below of=processPhaseSearch3, text width=20em, node distance=1.5cm] (processPhaseSearch4) {favor phase word in body by frequency:\\ $w_{phase}=w_{phase,page}\times(1.15\times tf_{phase,page})$};

            \node [block, below of=processPhaseSearch4, text width=20em, node distance=1.5cm] (processPhaseSearch5) {favor phase word in title by frequency:\\ $w_{phase,page}=w_{phase,page}\times(2\times tf_{phase\_in\_title,page})$};



            \node [block, right of=phaseDecide, node distance=4cm] (normalSearch) {normal search};
            \node [block, below of=normalSearch, text width=20em, node distance = 2.5cm] (processNormalSearch1) {calculate the term weight of the page\\ $w_{term}=(0.5+0.5\frac{tf_{term,page}}{max(tf_{page})})\times$ $\log_2(1+\frac{N_{num\_indexed\_page}}{df_{num\_page\_contain\_term}})$ \\ Zero if the term havent appear in the page};

            \node [block, below of=processNormalSearch1, text width=20em, node distance = 2.5cm] (processNormalSearch2) {favor query word in title by frequency:\\ $w_{term}=w_{term,page}\times(1.25\times tf_{term\_in\_title,page})$};

            \node [block, below of=phaseDecide, text width=25em, node distance = 11.1cm] (pageWeight) {calculate the pages weight:\\ $w_{page}=\sum_{term\in query}w_{term,query}\times w_{term,page}$};

            \node [block, below of=pageWeight, text width=47em,, node distance = 1.5cm, right=-7.5cm] (normalize) {normalize the page score by vector length:\\ $CosSim(page, query)=\frac{w_{page}}{query length \times Document length}$

            Notice that the CosSim may not be in range $[0,1]$ since favoring title and phase may cause the score $>$ 1.};

            \node [block, below of=normalize, text width=25em, node distance = 1.5cm] (sort) {sort the page by score and \\ return \{(UUID)page1=$score_{page1}\dots$\}};

            \node [cloud, below of=sort, node distance = 1.5cm] (output) {output the result};
            
            % Draw edges
            \path [line] (input) -- (stomstem);
            \path [line] (stomstem) -- (queryTerm);
            \path [line] (queryTerm) -- (phaseDecide);
            \path [line,dashed] (queryTerm) -- (queryTermException);
            \path [line] (queryTermException) -- (noResultEndding);
            
            \path [line] (phaseDecide) -- node {yes} (phaseSearch);
            \path [line] (phaseSearch) -- (processPhaseSearch1);
            \path [line] (processPhaseSearch1) -- (processPhaseSearch2);
            \path [line] (processPhaseSearch2) -- (processPhaseSearch3);
            \path [line] (processPhaseSearch3) -- (processPhaseSearch4);
            \path [line] (processPhaseSearch4) -- (processPhaseSearch5);
            \path [line] (processPhaseSearch5) -- (pageWeight);

            \path [line] (phaseDecide) -- node {no} (normalSearch);
            \path [line] (normalSearch) -- (processNormalSearch1);
            \path [line] (processNormalSearch1) -- (processNormalSearch2);
            \path [line] (processNormalSearch2) -- (pageWeight);

            \path [line] (pageWeight) -- (normalize);
            \path [line] (normalize) -- (sort);
            \path [line] (sort) -- (output);

        \end{tikzpicture}
    \end{center}

    \section*{Running the user interface (JSP web)}
        \subsection*{lib}
        Make sure those .jar are added the the lib of JSP (green .jar are newly added):
        \begin{figure}[!htbp]
            \centering
            \includegraphics[height=5cm]{jsplib.png}
            \caption{lib of JSP should contain those .jar}
        \end{figure}

        The .jar files can be copied from the lib folder of the project.
        \subsection*{webapps}
        Make sure the Project folder is in the webapps folder of Tomcat.
        \begin{figure}[!htbp]
            \centering
            \includegraphics[height=5cm]{webinf.png}
            \caption{The Project folder}
        \end{figure}

        The package under WEB-INF/classes can be copy from the compiled bin folder from figure 1. 

        \subsection*{DataBase}
        The databases should be placed in the same folder as the project folder (for my case).
        Even if you don't know where should the database be placed or don't have the databases, you can use the bonus features - Crawler in JSP web interface to index more pages to create databases and crawl pages. Details can be found in the Bonus features beyond requirement section.
        \begin{figure}[!htbp]
            \centering
            \includegraphics[height=5cm]{filesLocations.png}
            \caption{The databases locations}
        \end{figure}

        \newpage
        \subsection*{Run}
            In the project folder, run the following command: \\[0.4em]
            \centerline{\textbf{apache-tomcat-10.1.6/bin/startup.sh}}
            Then, open the browser and go to the following url: \\[0.4em]
            \centerline{\textbf{http://localhost:8080/Project/project.html}}

        \subsection*{Usage and show test}
        Type the query in the search box and click search.
        \begin{figure}[!htbp]
            \centering
            \includegraphics[width=10cm]{search.png}
            \caption{Search box}
        \end{figure}

        The result will be shown in the following format with your query words and phase shown:
        \begin{figure}[!htbp]
            \centering
            \includegraphics[width=14cm]{example1.png}
            \caption{Result}
        \end{figure}



    \newpage
    \section*{Bonus features beyond requirement}
        \subsection*{Query length}
            The query length don't have a maximum. It can be any length.    
        \subsection*{Phase length}
            The phase length is not limited to 2 or 3. It can be any length. Of course, the longer the phase, the less the result contain matches.
        
        \subsection*{Query weight}
            The query weight is not 1. It is calculated by the following formula:
            \begin{equation*}
                w_{term/phase,query}=\frac{tf_{term,query}}{max(tf_{query})}\times\log_2(1+\frac{N_{num\_indexed\_page}}{df_{num\_page\_contain\_term}})
            \end{equation*}
            It can show the importance of the term in the query. For example, a search query 'definition of quantum in quantum mechanics', the term 'quantum' will be larger than others in the $tf$ part. Also, the $idf$ part also indicate the likeliness of this term appear in a doc, the less it appear, the higher the weighting. Therefore, we can obtain the query weight which showing the level of importance of term in the query thus make the result more ranking more reasonable(favor the terms that are more specific and meaningful but not general).

        \subsection*{Crawling time is quite fast}
        It take less than 1 minute to crawl 300 pages. You can test about it. But please use CMD to run the crawler as JSP web interface will slow down the crawling speed :).
        \begin{figure}[!h]
            \centering
            \label{}
            \includegraphics[width=15cm]{crawl_in_local.png}
            \caption{Result of crawler in the command line}
        \end{figure}
        \subsection*{Special query are handled}
            \begin{enumerate}
                \item No match of the phases in the query, e.g. "BoscoLeung"
                \item One of the phase are all stop words, e.g. "the of", it will consider as no match.
                \item All of your query words are not indexed, e.g. BoscoLeung from hkustCSDepartment
                \item Query words are all stop words, e.g. there is a
            \end{enumerate}
            \begin{figure}[!htbp]
                \centering
                \label{}
                \includegraphics[width=10cm]{noResult.png}
                \caption{The web interface showing no result}
            \end{figure}

        \subsection*{Crawler in JSP web interface to index more pages}
            You can now use the crawler in the JSP web interface to index more pages. Even you don't have the database at first. You can now index pages from the interface.
            \begin{figure}[!htbp]
                \centering
                \label{}
                \includegraphics[width=17cm]{crawl_in_web.png}
                \caption{Demo of the crawler in JSP web interface}
            \end{figure}

            But noticed that the crawling speed will be slower compare to that of crawling in the command line because of the JSP stuff. And the process will not be shown in the web interface. So better not to crawl too many pages at once :). Or you need to panient to wait for the result. (The page crawled will in the datebase even hard stopped.)
            \begin{figure}[!h]
                \centering
                \label{}
                \includegraphics[width=15cm]{crawl_in_local.png}
                \caption{Demo of the crawler in the command line}
            \end{figure}

            \newpage
            Example: 
            \begin{figure}[!htbp]
                \centering
                \includegraphics[width=6cm]{c1.png}
                \caption{A page that is not indexed}
            \end{figure}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[width=9cm]{c2.png}
                \caption{The starting page}
            \end{figure}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[width=15cm]{c3.png}
                \caption{Enter the crawling information}
            \end{figure}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[width=15cm]{c4.png}
                \caption{After crawling}
            \end{figure}
            \begin{figure}[!htbp]
                \centering
                \includegraphics[width=9cm]{c5.png}
                \caption{The page is now indexed and searchable}
            \end{figure}
            \newpage
            End of example.
    \section*{Conclusion}
        \subsection*{Strengths}
        The searching time is fast even there will be 50 results to output. \\
        The searching result is shown in a good format. \\
        The web interface is easy to use and responsive. 
        \subsection*{Weaknesses}
        \subsubsection*{Phase search minor defect}
        Some time the phase search result is not good because of the linking postition of the title and the body.
        \begin{figure}[!htbp]
            \centering
            \includegraphics[height=6cm]{bad2.png}
            \caption{Example of bad phase search result}
        \end{figure}

        \begin{figure}[!htbp]
            \centering
            \includegraphics[width=6cm]{bad3.png}
            \caption{website of the bad phase search result}
        \end{figure}

        The search result may first give a seemmingly not accurate result, as no phase "hkust cse" is found. But when look into the html source code of the page. The word hkust in the last of the title and cse in the first of the body are consider as a phase as their position are in oder. 

        \begin{figure}[!htbp]
            \centering
            \includegraphics[width=15cm]{bad1.png}
            \caption{Reason of bad phase search result}
        \end{figure}
        
        This is because in my index, there are two type of index, one is for the title and one is for the title and body.
        So this kind of unusual case will happen in rare cases but the effect is small as those two word is not usually to make a meaning phase. \\

        Therefore, if I could re-implement the whold system, I will seperate the title and body into two different index. So that the phase search result will be more accurate. 

        \subsubsection*{Links not utilized in the ranking}
        The links relationships are not used for the searching. It maybe useful to perform PageRank, HITS or other ranking algorithm to rank the result and make the result more accurate.

    \section*{Special Notice}
        \subsection*{Word Extraction}
            I have set 
            \begin{lstlisting}[language=Java]
                sb.setLinks(false)
            \end{lstlisting} 
            in the word extraction part, which is different from the lab. \\[0.4em] 
            Since if it is set to true, it will create some keywords like $httpslibraryhkusteduhkaboutushoursservicepointshoursservic$ and $httpslibraryhkusteduhkhelpforalumnialumni$, it doesn't seem to make sense. 
        \subsection*{Crawler Strategy}
            I have implemented the BFS strategy in this phase. And the crawler will pick the next URL according the occurrence order in the webpage.
        \subsection*{Page Last Modified Date and Page Size}
            Currently I am using the following method to get the last modified date of the page:
            \begin{lstlisting}[language=Java]
                url.openConnection().getLastModified();
            \end{lstlisting}
            But it seems that the last modified date may missing. In this case, the last modified date will be set to 
            \begin{lstlisting}[language=Java]
                url.openConnection().getDate();
            \end{lstlisting}
            which is the date of accessing. \\[0.4em]
            For the page size, I am using the following method to get the page size:
            \begin{lstlisting}[language=Java]
                url.openConnection().getContentLength();
            \end{lstlisting}
            If the page size is missing, I will use the page size value method obtain in lab 2 instead.
        
            
\end{document}